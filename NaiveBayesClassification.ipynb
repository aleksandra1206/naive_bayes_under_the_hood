{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae4b7fe-ec3f-4d81-a0da-9777b2d8b590",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "PROJECT NAME: NAIVE BAYES UNDER THE HOOD\n",
    "\n",
    "PROJECT GOAL:\n",
    "\n",
    "The primary objective of this project is to demystify the Naive Bayes classifier, presenting it in an accessible manner that anyone can understand. It aims to break down the fundamental concepts behind this powerful machine learning algorithm, illustrating how it works and providing a step-by-step implementation at a basic level. By the end of this project, readers will have a clear understanding of the Naive Bayes classifier's principles, applications, and practical implementation, empowering them to apply these insights in real-world scenarios.\n",
    "\n",
    "DESCRIPTION:\n",
    "\n",
    "This project focuses on implementing a robust Naive Bayes classifier using Python, \n",
    "building upon the foundational concepts of probability theory and statistical inference. \n",
    "At its core, the Naive Bayes algorithm leverages Bayes' theorem, which relates the conditional and marginal probabilities of random variables, \n",
    "to make predictions based on input features. \n",
    "\n",
    "To enhance the model's reliability, we incorporate logarithmic transformation to address potential underflow issues \n",
    "that can arise during probability calculations, especially when dealing with very small values. \n",
    "When probabilities are multiplied together, as is common in Naive Bayes, the resulting product can quickly become exceedingly small, \n",
    "potentially falling below the range of standard floating-point representation and causing numerical instability. \n",
    "By applying logarithms, we transform these multiplications into sums, effectively stabilizing the program and eliminating the risk of underflow \n",
    "while maintaining the accuracy of our predictions.\n",
    "\n",
    "Moreover, we introduce additive smoothing, a technique that mitigates the problem of zero probabilities when estimating class-conditional probabilities. \n",
    "This functionality allows users to specify the degree of smoothing, enhancing the model's performance in scenarios with limited training data \n",
    "or unseen features. By adding a small constant (smoothing parameter) to the frequency counts, we ensure that no probability is ever exactly zero, \n",
    "allowing the algorithm to gracefully handle unseen attribute values during prediction.\n",
    "\n",
    "Finally, the project expands its applicability by enabling the Naive Bayes algorithm to work with numerical input attributes. \n",
    "Instead of treating these attributes as categorical, we model their conditional distribution using a normal (Gaussian) distribution. \n",
    "This approach entails estimating the mean and standard deviation for each numerical feature within each class, \n",
    "thereby allowing us to compute probabilities based on the properties of the normal distribution. \n",
    "Consequently, the classifier can effectively handle a mixture of categorical and continuous features, \n",
    "making it a versatile tool for various classification tasks. \n",
    "\n",
    "Through these enhancements, the project aims to provide a comprehensive implementation of the Naive Bayes classifier that balances mathematical rigor \n",
    "with practical usability.\n",
    "\n",
    "DATA OVERVIEW:\n",
    "\n",
    "The training data consists of 200 entries with the following attributes:\n",
    "Age: Integer values (e.g., 23, 47)\n",
    "Sex: Categorical values (F, M)\n",
    "BP: Categorical values (HIGH, LOW, NORMAL)\n",
    "Cholesterol: Categorical values (HIGH, NORMAL)\n",
    "Na: Continuous values (float)\n",
    "K: Continuous values (float)\n",
    "Drug: Target variable (categorical with classes drugY, drugX, drugA, drugC, drugB)\n",
    "\n",
    "USED PYTHON LIBRARIES: pandas, numpy, scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d610230-b162-4e4c-9f30-f8ec8dbebe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importing the pandas library for data manipulation and analysis\n",
    "import numpy as np  # Importing NumPy for numerical operations\n",
    "from scipy.stats import norm  # Importing the normal distribution from scipy.stats\n",
    "\n",
    "# Function to learn the Naive Bayes model\n",
    "def learn(data, class_att, smoothing=1e-5): \n",
    "    model = {}  # Initialize an empty dictionary to store the model\n",
    "\n",
    "    # Calculate the prior probabilities of each class\n",
    "    apriori = data[class_att].value_counts(normalize=True)\n",
    "    model['_apriori'] = apriori \n",
    "\n",
    "    # Loop through each attribute in the dataset, excluding the class attribute\n",
    "    for attribute in data.drop(class_att, axis=1).columns:\n",
    "        if data[attribute].dtype == 'object':  # Check if the attribute is categorical\n",
    "            # Create a contingency table (counts of each category for each class) with smoothing to avoid zero probabilities\n",
    "            mat_kont = pd.crosstab(data[attribute], data[class_att]) + smoothing\n",
    "            mat_kont = mat_kont / mat_kont.sum(axis=0)  # Normalize the counts to get probabilities\n",
    "            model[attribute] = np.log(mat_kont)  # Store the log of the probabilities in the model to avoid underflow\n",
    "        else:\n",
    "            # For numerical attributes, calculate the mean and standard deviation for each class\n",
    "            mean_std = data.groupby(class_att)[attribute].agg(['mean', 'std'])\n",
    "            model[attribute]= mean_std  # Store the mean and std in the model\n",
    "    return model  # Return the trained model\n",
    "\n",
    "# Function to predict the class of new instances\n",
    "def predict(model, new_instance):\n",
    "    class_probabilities = {}  # Initialize a dictionary to store class probabilities\n",
    "    \n",
    "    # Loop through each class in the model\n",
    "    for class_value in model['_apriori'].index:\n",
    "        probability = 0  # Initialize the probability for the current class\n",
    "        \n",
    "        # Loop through each attribute in the model\n",
    "        for attribute in model:\n",
    "            if attribute == '_apriori':  # If the attribute is the prior probabilities\n",
    "                probability += np.log(model['_apriori'][class_value])  # Add the log prior probability\n",
    "            else:\n",
    "                # Check if the attribute is categorical and exists in the new instance\n",
    "                if isinstance(model[attribute], pd.DataFrame):\n",
    "                    if new_instance[attribute] in model[attribute].index:\n",
    "                        probability += model[attribute].loc[new_instance[attribute], class_value]  # Add log probability\n",
    "                    else:\n",
    "                        probability += np.log(1e-5)  # Add a small value for unseen categories\n",
    "                else: \n",
    "                    # For numerical attributes, calculate the probability using the normal distribution\n",
    "                    mean, std = model[attribute].loc[class_value]\n",
    "                    probability += np.log(norm.pdf(new_instance[attribute], mean, std))  # Add log of Gaussian probability\n",
    "                    \n",
    "        class_probabilities[class_value] = probability  # Store the calculated probability for the class\n",
    "\n",
    "    # Get the class with the highest probability\n",
    "    prediction = max(class_probabilities, key=class_probabilities.get)\n",
    "\n",
    "    return prediction, class_probabilities  # Return the predicted class and its probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "550c9e9d-4548-4d5f-af99-929e77b1c26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA (FIRST 10 ROWS): \n",
      "   Age Sex      BP Cholesterol        Na         K   Drug\n",
      "0   23   F    HIGH        HIGH  0.792535  0.031258  drugY\n",
      "1   47   M     LOW        HIGH  0.739309  0.056468  drugC\n",
      "2   47   M     LOW        HIGH  0.697269  0.068944  drugC\n",
      "3   28   F  NORMAL        HIGH  0.563682  0.072289  drugX\n",
      "4   61   F     LOW        HIGH  0.559294  0.030998  drugY\n",
      "5   22   F  NORMAL        HIGH  0.676901  0.078647  drugX\n",
      "6   49   F  NORMAL        HIGH  0.789637  0.048518  drugY\n",
      "7   41   M     LOW        HIGH  0.766635  0.069461  drugC\n",
      "8   60   M  NORMAL        HIGH  0.777205  0.051230  drugY\n",
      "9   43   M     LOW      NORMAL  0.526102  0.027164  drugY\n",
      "\n",
      "BASIC DATAFRAME INFO: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Age          200 non-null    int64  \n",
      " 1   Sex          200 non-null    object \n",
      " 2   BP           200 non-null    object \n",
      " 3   Cholesterol  200 non-null    object \n",
      " 4   Na           200 non-null    float64\n",
      " 5   K            200 non-null    float64\n",
      " 6   Drug         200 non-null    object \n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 11.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from a CSV file\n",
    "data = pd.read_csv('C:/Users/Korisnik/Downloads/BIG DATA/BIG DATA/DOMACI 1/data/drug.csv') # local path on my PC\n",
    "\n",
    "print(\"TRAINING DATA (FIRST 10 ROWS): \")\n",
    "print(data.head(10))\n",
    "print()\n",
    "print(\"BASIC DATAFRAME INFO: \")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61380e2-2af7-4d93-8986-44fa646496dd",
   "metadata": {},
   "source": [
    "- The dataset contains 200 entries and consists of 7 columns, which include both categorical and numerical features. \n",
    "\n",
    "- Columns:\n",
    "\n",
    "  * Age: An integer column representing the age of the individuals. It is a numerical value.\n",
    "\n",
    "  * Sex: A categorical column (with values 'M' and 'F') indicating the gender of the individuals. This feature can help capture any gender-related     trends in drug prescriptions.\n",
    "\n",
    "  * BP: A categorical column indicating blood pressure levels with three possible values: 'HIGH', 'LOW', and 'NORMAL'. This information may play a   significant role in determining drug prescriptions based on blood pressure conditions.\n",
    "\n",
    "  * Cholesterol: Another categorical column that describes cholesterol levels, also with values 'HIGH' and 'NORMAL'. \n",
    "\n",
    "  * Na: A float column representing sodium levels in the blood, measured as a continuous variable. This feature may help in predicting drug type based on biochemical factors.\n",
    "\n",
    "  * K: A float column representing potassium levels, similar to the sodium column. This is also a continuous variable that may influence drug predictions.\n",
    "\n",
    "  * Drug: Cathegorical variable. Indicating the prescribed drug type, with multiple classes (e.g., 'drugY', 'drugX', 'drugC', etc.). This is the outcome variable that the model aims to predict.\n",
    "\n",
    "- There are no missing values in the dataframe.\n",
    "\n",
    "- Target variable: 'Drug' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba491500-39da-4615-8138-00be1217e880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET VARIABLE ('Drug') INFO: \n",
      "NUMBER OF CLASSES: 5, CLASS LABELS:  ['drugY' 'drugC' 'drugX' 'drugA' 'drugB']\n",
      "VALUE COUNTS: \n",
      "Drug\n",
      "drugY    91\n",
      "drugX    54\n",
      "drugA    23\n",
      "drugC    16\n",
      "drugB    16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"TARGET VARIABLE ('Drug') INFO: \")\n",
    "distinct_values = data['Drug'].unique()\n",
    "value_counts = len(distinct_values)\n",
    "print(f\"NUMBER OF CLASSES: {value_counts}, CLASS LABELS: \", distinct_values)\n",
    "print(\"VALUE COUNTS: \")\n",
    "print(data['Drug'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7598cbab-571e-46b8-b760-1691d89571f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Naive Bayes model...\n",
      "\n",
      "MODEL: \n",
      "{'_apriori': Drug\n",
      "drugY    0.455\n",
      "drugX    0.270\n",
      "drugA    0.115\n",
      "drugC    0.080\n",
      "drugB    0.080\n",
      "Name: proportion, dtype: float64, 'Age':             mean        std\n",
      "Drug                       \n",
      "drugA  35.869565   9.696786\n",
      "drugB  62.500000   7.127412\n",
      "drugC  42.500000  16.725230\n",
      "drugX  44.018519  16.435685\n",
      "drugY  43.747253  17.031731, 'Sex': Drug     drugA     drugB     drugC     drugX     drugY\n",
      "Sex                                                   \n",
      "F    -0.693397 -0.693347 -0.693247 -0.693147 -0.692998\n",
      "M    -0.692897 -0.692947 -0.693047 -0.693147 -0.693297, 'BP': Drug       drugA     drugB     drugC     drugX     drugY\n",
      "BP                                                      \n",
      "HIGH   -1.097081 -1.097547 -1.099145 -1.100411 -1.097848\n",
      "LOW    -1.099379 -1.099145 -1.097547 -1.098612 -1.098646\n",
      "NORMAL -1.099379 -1.099145 -1.099145 -1.096817 -1.099344, 'Cholesterol': Drug            drugA     drugB     drugC     drugX     drugY\n",
      "Cholesterol                                                  \n",
      "HIGH        -0.693097 -0.693147 -0.692348 -0.693846 -0.692998\n",
      "NORMAL      -0.693197 -0.693147 -0.693947 -0.692449 -0.693297, 'Na':            mean       std\n",
      "Drug                     \n",
      "drugA  0.662492  0.124366\n",
      "drugB  0.727980  0.086082\n",
      "drugC  0.675556  0.114466\n",
      "drugX  0.652180  0.113085\n",
      "drugY  0.730851  0.116670, 'K':            mean       std\n",
      "Drug                     \n",
      "drugA  0.061939  0.011621\n",
      "drugB  0.064132  0.009061\n",
      "drugC  0.064743  0.010442\n",
      "drugX  0.062785  0.011498\n",
      "drugY  0.034701  0.009891}\n"
     ]
    }
   ],
   "source": [
    "output_var = 'Drug'  # Specify the target variable (class attribute)\n",
    "\n",
    "print(\"\\nTraining Naive Bayes model...\")\n",
    "# Train the model using the learn function with a specified smoothing parameter\n",
    "model = learn(data, output_var, 10000)\n",
    "print(\"\\nMODEL: \")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693917ee-9117-4515-910e-98e4bbeff939",
   "metadata": {},
   "source": [
    "- The _apriori section gives the prior probabilities of each class (Drug type) in the dataset. These probabilities represent how likely each drug is to occur in the absence of any other information and they are calculated as \n",
    "number of appearances for each class divided by a total number of entries (200):\n",
    "drugY: 0.455\n",
    "drugX: 0.270\n",
    "drugA: 0.115\n",
    "drugC: 0.080\n",
    "drugB: 0.080\n",
    "- This means that drugY is the most common class, appearing in 45.5% of the data, while drugB and drugC are the least common.\n",
    "- When it comes to numerical features like 'Age' for example, model contains the mean and std for every class label when it comes to that feature:\n",
    "'Age':    mean        std\n",
    "Drug                       \n",
    "drugA  35.869565   9.696786\n",
    "drugB  62.500000   7.127412\n",
    "drugC  42.500000  16.725230\n",
    "drugX  44.018519  16.435685\n",
    "drugY  43.747253  17.031731\n",
    "- When it comes to cathegorical features like 'BP' for example, we calculated the probability of a specific BP category given that a patient is taking a certain drug (CONDITIONAL PROBABILITY). Each entry corresponds to the log probability of having a specific BP level (HIGH, LOW, NORMAL) when a particular drug is being taken (drugA, drugB, etc.), calculated by a log of the conditional probability - log(P(BP∣Drug)):\n",
    "BP\n",
    "Drug       drugA     drugB     drugC     drugX     drugY\n",
    "HIGH   -1.097081 -1.097547 -1.099145 -1.100411 -1.097848\n",
    "LOW    -1.099379 -1.099145 -1.097547 -1.098612 -1.098646\n",
    "NORMAL -1.099379 -1.099145 -1.099145 -1.096817 -1.099344\n",
    "The values for these features are negative, indicating logged probabilities. The closer the log probability is to zero (less negative), the higher the corresponding probability for that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee2f31-4f31-469c-88ac-2672bb1bb96f",
   "metadata": {},
   "source": [
    "- The _apriori section gives the prior probabilities of each class (Drug type) in the dataset. These probabilities represent how likely each drug is to occur in the absence of any other information:\n",
    "drugY: 0.455\n",
    "drugX: 0.270\n",
    "drugA: 0.115\n",
    "drugC: 0.080\n",
    "drugB: 0.080\n",
    "- This means that drugY is the most common class, appearing in 45.5% of the data, while drugB and drugC are the least common.\n",
    "- When it comes to numerical features like 'Age' for example, we got mean and std of Age for each class\n",
    "- When it comes to cathegorical features like 'BP', we calculated the probability of a specific BP category given that a patient is taking a certain drug. Each entry in the table corresponds to the log probability of having a specific BP level (HIGH, LOW, NORMAL) when a particular drug is being taken (drugA, drugB, etc.), calculated by formula log(P(BP/Drug)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "416d4fe5-c466-4b46-8824-22cd04d0399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS FOR NEW INSTANCES:\n",
      "Instance:  {'Age': 10, 'Sex': 'F', 'BP': 'HIGH', 'Cholesterol': 'HIGH', 'Na': 0.65222, 'K': 0.4}\n",
      "Prediction: drugY\n",
      "Confidence: {'drugY': -37.81007822966299, 'drugX': -38.3355131055089, 'drugA': -39.185175011891516, 'drugC': -39.54924576403176, 'drugB': -39.5485460189035}\n",
      "\n",
      "Instance:  {'Age': 25, 'Sex': 'F', 'BP': 'LOW', 'Cholesterol': 'LOW', 'Na': 0.222, 'K': 0.3}\n",
      "Prediction: drugY\n",
      "Confidence: {'drugY': -48.63080311277627, 'drugX': -49.15279464909273, 'drugA': -50.007300596669445, 'drugC': -50.368224368271484, 'drugB': -50.36992302467748}\n",
      "\n",
      "Instance:  {'Age': 60, 'Sex': 'M', 'BP': 'HIGH', 'Cholesterol': 'HIGH', 'Na': 0.66, 'K': 0.061939}\n",
      "Prediction: drugY\n",
      "Confidence: {'drugY': -37.810376870847826, 'drugX': -38.3355131055089, 'drugA': -39.18467558622065, 'drugC': -39.5490459239032, 'drugB': -39.54814633864238}\n",
      "\n",
      "Instance:  {'Age': 35, 'Sex': 'M', 'BP': 'HIGH', 'Cholesterol': 'HIGH', 'Na': 0.66, 'K': 0.061939}\n",
      "Prediction: drugY\n",
      "Confidence: {'drugY': -37.810376870847826, 'drugX': -38.3355131055089, 'drugA': -39.18467558622065, 'drugC': -39.5490459239032, 'drugB': -39.54814633864238}\n"
     ]
    }
   ],
   "source": [
    "# Prediction instances\n",
    "new_instance = {'Age': 10, 'Sex': 'F', 'BP': 'HIGH', 'Cholesterol': 'HIGH', 'Na': 0.65222, 'K':0.4}\n",
    "new_instance2 = {'Age': 25, 'Sex': 'F', 'BP': 'LOW', 'Cholesterol': 'LOW', 'Na': 0.222, 'K':0.3}\n",
    "new_instance3 = {'Age': 60, 'Sex': 'M', 'BP': 'HIGH', 'Cholesterol': 'HIGH', 'Na': 0.66, 'K':0.061939}\n",
    "new_instance4 = {\n",
    "    'Age': 35,\n",
    "    'Sex': 'M',\n",
    "    'BP': 'HIGH',\n",
    "    'Cholesterol': 'HIGH',\n",
    "    'Na': 0.66,\n",
    "    'K': 0.061939,\n",
    "}\n",
    "\n",
    "# Make predictions for each new instance and print the results\n",
    "prediction, confidence = predict(model, new_instance)\n",
    "prediction2, confidence2 = predict(model, new_instance2)\n",
    "prediction3, confidence3 = predict(model, new_instance3)\n",
    "prediction4, confidence4 = predict(model, new_instance4)\n",
    "\n",
    "print(\"PREDICTIONS FOR NEW INSTANCES:\")\n",
    "print(\"Instance: \", new_instance)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Confidence:\", confidence)\n",
    "\n",
    "print(\"\\nInstance: \", new_instance2)\n",
    "print(\"Prediction:\", prediction2)\n",
    "print(\"Confidence:\", confidence2)\n",
    "\n",
    "print(\"\\nInstance: \", new_instance3)\n",
    "print(\"Prediction:\", prediction3)\n",
    "print(\"Confidence:\", confidence3)\n",
    "\n",
    "print(\"\\nInstance: \", new_instance4)\n",
    "print(\"Prediction:\", prediction4)\n",
    "print(\"Confidence:\", confidence4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e043c45-549f-4a23-abfd-9c474002283f",
   "metadata": {},
   "source": [
    "ANALYSIS AND CONCLUSION:\n",
    "\n",
    "The Naive Bayes classifier consistently predicts the drugY class, highlighting the challenges posed by imbalanced class distributions in our dataset. The predominance of drugY in the training data leads the model to favor this class, as it has learned to prioritize the most frequently occurring category over others.\n",
    "\n",
    "It is important to note that this project focuses primarily on explaining the basic Naive Bayes algorithm, which is why the topic of balancing classes has been left out of our implementation. However, to mitigate the effects of class imbalance in real-world applications, practitioners can consider several strategies:\n",
    "\n",
    "- Resampling Techniques: Implement methods such as oversampling the minority class or undersampling the majority class to create a more balanced training dataset.\n",
    "- Cost-Sensitive Learning: Adjust the learning algorithm to give more weight to the minority class, helping to improve its predictive performance.\n",
    "- Evaluation Metrics: Utilize metrics beyond accuracy, such as precision, recall, and F1-score, to gain a better understanding of the model's performance across all classes.\n",
    "\n",
    "By being mindful of class imbalance and employing appropriate techniques, we can enhance the model's ability to generalize across all classes, ultimately leading to more reliable and equitable predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
